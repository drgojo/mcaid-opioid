{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc, f1_score, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nevada Features\n",
    "STATE = \"***Final\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == '01').astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "***_features = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***_features.loc[:,'STATE'] = 1\n",
    "***_features = ***_features.loc[***_features.opioid_disorder <= 0]\n",
    "***_features = ***_features.loc[***_features.AGE <= 65]\n",
    "***_features = ***_features.loc[***_features.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Features\n",
    "STATE = \"***Final\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == 1).astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "***_features = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***features.loc[:,'STATE'] = 2\n",
    "***_features = ***_features.loc[colorado_features.opioid_disorder <= 0]\n",
    "***_features = ***_features.loc[colorado_features.AGE <= 65]\n",
    "***_features = ***features.loc[colorado_features.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Features\n",
    "STATE = \"***\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == '01').astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "*** = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***.loc[:,'STATE'] = 3\n",
    "*** = ***.loc[***.opioid_disorder <= 0]\n",
    "*** = ***.loc[***.AGE <= 65]\n",
    "*** = ***.loc[westvirginia_features.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nevada Features\n",
    "STATE = \"FloridaFinal\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == '01').astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "*** = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***.loc[:,'STATE'] = 4\n",
    "*** = ***.loc[***.opioid_disorder <= 0]\n",
    "*** = ***.loc[***.AGE <= 65]\n",
    "*** = ***.loc[***.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Features\n",
    "STATE = \"***\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == '01').astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "*** = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***.loc[:,'STATE'] = 5\n",
    "*** = ***.loc[***.opioid_disorder <= 0]\n",
    "*** = ***.loc[***.AGE <= 65]\n",
    "*** = ***.loc[***.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Features\n",
    "STATE = \"***\"\n",
    "init_opioid_features = pd.read_csv(STATE + \"/init_opioid_features.csv\")\n",
    "dx_features = pd.read_csv(STATE + \"/dx_features.csv\")\n",
    "eligibility_features = pd.read_csv(STATE + \"/eligibility_features.csv\")\n",
    "pre_common_features = pd.read_csv(STATE + \"/pre_common_features.csv\")\n",
    "pre_rx_features = pd.read_csv(STATE + \"/pre_rx_features.csv\")\n",
    "dx_filter_init_feature = pd.read_csv(STATE + \"/dx_filter_init_feature.csv\")\n",
    "\n",
    "full_features = init_opioid_features.merge(dx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(eligibility_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_common_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(pre_rx_features.drop(\"OUTCOME\",axis = 1), how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "full_features = full_features.merge(dx_filter_init_feature, how = \"left\", left_on = \"MA_NUM\", right_on = \"MA_NUM\")\n",
    "\n",
    "dx_columns = dx_features.drop([\"OUTCOME\",\"MA_NUM\"],axis = 1).columns.append(dx_filter_init_feature.drop([\"MA_NUM\"],axis = 1).columns)\n",
    "full_features.loc[:,dx_columns] = full_features.loc[:,dx_columns].fillna(0).astype(bool)\n",
    "\n",
    "full_features = full_features.loc[full_features.ELIGIBILITY_TIME > 30*2]\n",
    "\n",
    "pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "opioid_naive_info_known = pickle.load(pickle_in)\n",
    "\n",
    "full_features.loc[:,\"Hispanic\"] = (full_features.ETHNICITY == '01').astype(float)\n",
    "\n",
    "matrix_features = full_features.drop([\"MA_NUM\",\"CLAIM_ID\",\"Generic_Drug_Name\",\"FIPS\",'ETHNICITY','LANGUAGE','INITIAL_PREG_IND','PREVIOUSLY_PREGNANT'],axis = 1)\n",
    "\n",
    "*** = pd.get_dummies(matrix_features, columns = ['DEAClassCode','INITIAL_CLAIM_TYPE','INITIAL_PLACE_OF_SERVICE','PROVIDER_TYPE','PROVIDER_SPECIALTY','PROVIDER_FILE_TYPE','Drug','LongShortActing','Master_Form',\"GENDER\"])\n",
    "***.loc[:,'STATE'] = 6\n",
    "*** = ***.loc[***.opioid_disorder <= 0]\n",
    "*** = ***.loc[***.AGE <= 65]\n",
    "*** = ***.loc[***.AGE >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = pd.concat([***.sample(n = 30000,replace = True, random_state = 42,axis = 'index'),\n",
    "                            ***.sample(n = 30000,replace = True, random_state = 42,axis = 'index'),\n",
    "                            ***.sample(n = 30000,replace = True, random_state = 42,axis = 'index'),\n",
    "                            ***.sample(n = 30000,replace = True, random_state = 42,axis = 'index'),\n",
    "                            ***.sample(n = 30000,replace = True, random_state = 42,axis = 'index'),\n",
    "                            ***.sample(n = 30000,replace = True, random_state = 42,axis = 'index')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = set(***.columns).intersection(set(***.columns),set(***.columns),set(***.columns),set(***.columns),set(***.columns))\n",
    "total_features = total_features.loc[:,overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All have mean < .001\n",
    "total_features = total_features.drop([\"InitLongTermCare\",\"metastatic_cancer\",\"solid_tumor_wo_metastasis\",\"InitIndependentClinic\",\"desipramine_ndcs\",\"buprenorphine_ndcs\",\"capsaicin_ndcs\",\n",
    "                                      \"InitTransportNonAmb\",\"InitRehabilitation\",\"InstitutionalNewborn\",\"dacomitinib_ndcs\",\"Master_Form_Capsule, Extended Release\",\"opioid_disorder\",\"ABUSE_DETER\",\"lymphoma\",\n",
    "                                     \"InitInpatient\", \"InstitutionalTraumaNotAvail\",\"init_diag_K04\",\"imipramine_ndcs\",\"INITIAL_RX_NDC_CODE\", \"diclofenac_ndcs_y\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any feautures that are present in <= 1% of records\n",
    "total_features = total_features.loc[:,total_features.fillna(0).mean() > .01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_graph = [total_features[total_features.INITIAL_RX_QUANTITY > i].OUTCOME.mean() for i in range(0,80,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(range(0,80,1), quant_graph)\n",
    "ax.set(xlabel =\"Quantity Prescribed\", ylabel = \"% COU\", title ='Percent COU by Quantity Cutoff')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_NaN_stats = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_stats = pd.DataFrame(fill_NaN_stats.fit_transform(total_features))\n",
    "imputed_stats.columns = total_features.columns\n",
    "imputed_stats.index = total_features.index\n",
    "stat_matrix = imputed_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby(\"STATE\").OUTCOME.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.OUTCOME.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby(\"OUTCOME\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby('init_diag_G89').OUTCOME.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(stat_matrix.loc[stat_matrix.init_diag_G89 == 0].INITIAL_RX_QUANTITY, stat_matrix.loc[stat_matrix.init_diag_G89 == 1].INITIAL_RX_QUANTITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(stat_matrix.loc[stat_matrix.init_diag_M54 == 0].INITIAL_RX_QUANTITY, stat_matrix.loc[stat_matrix.init_diag_M54 == 1].INITIAL_RX_QUANTITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby('init_diag_G89').INITIAL_RX_QUANTITY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby('init_diag_M54').INITIAL_RX_QUANTITY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.loc[stat_matrix.init_diag_G89 == 1].groupby(\"OUTCOME\").INITIAL_RX_QUANTITY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.loc[stat_matrix.init_diag_G89 == 1].groupby(\"OUTCOME\").INITIAL_RX_QUANTITY.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.loc[stat_matrix.init_diag_G89 == 1].groupby(\"OUTCOME\").INITIAL_RX_QUANTITY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(stat_matrix.loc[(stat_matrix.init_diag_G89 == 1) &(stat_matrix.OUTCOME == 0)].INITIAL_RX_QUANTITY, stat_matrix.loc[(stat_matrix.init_diag_G89 == 1)&(stat_matrix.OUTCOME == 1)].INITIAL_RX_QUANTITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.loc[stat_matrix.init_diag_M54 == 1].groupby(\"OUTCOME\").INITIAL_RX_QUANTITY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(stat_matrix.loc[(stat_matrix.init_diag_M54 == 1) &(stat_matrix.OUTCOME == 0)].INITIAL_RX_QUANTITY, stat_matrix.loc[(stat_matrix.init_diag_M54 == 1)&(stat_matrix.OUTCOME == 1)].INITIAL_RX_QUANTITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "\n",
    "st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.columns[stat_matrix.mean() < .001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby(\"STATE\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hispanic stats\n",
    "print(stat_matrix.loc[stat_matrix.STATE.isin([1,2])].Hispanic.mean())\n",
    "print(stat_matrix.loc[stat_matrix.STATE.isin([1,2])].groupby(\"OUTCOME\").Hispanic.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.MED.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.MED.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(stat_matrix.MED == stat_matrix.MED.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.loc[stat_matrix.MED < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby([\"STATE\",\"OUTCOME\"]).mean().T.to_csv(\"final_means.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby([\"OUTCOME\"]).mean().T.drop(\"STATE\").to_csv(\"final_means_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.groupby([\"STATE\",\"OUTCOME\"]).median().T.to_csv(\"final_medians.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"p_value\":ttest_ind(stat_matrix.loc[stat_matrix.OUTCOME == 0].drop([\"STATE\",\"OUTCOME\"],axis = 1),stat_matrix.loc[stat_matrix.OUTCOME == 1].drop([\"STATE\",\"OUTCOME\"],axis = 1)).pvalue}).to_csv(\"variable_pvals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.to_csv(\"raw_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for var in ['AGE','PRE_RX_QTY_SUM','TOTAL_RXS','INITIAL_RX_AMOUNT_BILLED','LongShortActing_LA',\n",
    "             'INITIAL_RX_QUANTITY',\n",
    "             'AGE',\n",
    "             'LongShortActing_LA',\n",
    "             'INITIAL_RX_LENGTH',\n",
    "             'pregabalin_ndcs',\n",
    "             'InitIndipendantLab',\n",
    "             'InitDental',\n",
    "             'emergency_room_claim',\n",
    "             'InstitutionalElective',\n",
    "             'Drug_Tramadol SA',\n",
    "             'GENDER_F']:\n",
    "    acute = stat_matrix.loc[stat_matrix.OUTCOME == 0].loc[:,var]\n",
    "    chronic = stat_matrix.loc[stat_matrix.OUTCOME == 1].loc[:,var]\n",
    "    plt.boxplot(x = [acute,chronic], labels = ['acute','chronic'], showfliers=False)\n",
    "    plt.grid( alpha=0.75)\n",
    "    plt.ylabel('{}'.format(var))\n",
    "    plt.title('{} by outcome'.format(var))\n",
    "    plt.show()\n",
    "    print(\"Mean Acute: {}\".format(np.mean(acute)))\n",
    "    print(\"Mean Chronic: {}\".format(np.mean(chronic)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stat_matrix.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_matrix.INITIAL_RX_QUANTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=1, sharey=True, figsize=(7, 4))\n",
    "fig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\n",
    "hb = ax.hexbin( stat_matrix.INITIAL_RX_QUANTITY, stat_matrix.MED,\n",
    "          mincnt = 50,\n",
    "          extent = (0,125,0,100),\n",
    "          gridsize = 10,\n",
    "          vmax = 10000)\n",
    "ax.set_title(\"Initial Quantity by MME Per Day\")\n",
    "ax.set_ylabel(\"MME per Day\")\n",
    "ax.set_xlabel(\"Quantity prescribed\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('counts')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=1, sharey=True, figsize=(7, 4))\n",
    "fig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\n",
    "hb = ax.hexbin(stat_matrix.INITIAL_RX_QUANTITY, stat_matrix.INITIAL_RX_LENGTH, \n",
    "          mincnt = 50,\n",
    "          extent = (0,100,0,80),\n",
    "          gridsize = 10,\n",
    "          vmax = 10000)\n",
    "ax.set_title(\"Initial Quantity by Days Prescribed\")\n",
    "ax.set_ylabel(\"Days Prescribed\")\n",
    "ax.set_xlabel(\"Quantity prescribed\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('counts')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaving out *** for full val set\n",
    "fifth_val = total_features.loc[total_features.STATE == 2]\n",
    "total_features = total_features.loc[total_features.STATE != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(total_features, stratify = total_features.OUTCOME,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_train = pd.DataFrame(fill_NaN.fit_transform(train))\n",
    "imputed_train.columns = train.columns\n",
    "imputed_train.index = train.index\n",
    "train = imputed_train\n",
    "\n",
    "imputed_test = pd.DataFrame(fill_NaN.transform(test))\n",
    "imputed_test.columns = test.columns\n",
    "imputed_test.index = test.index\n",
    "test = imputed_test\n",
    "\n",
    "imputed_fifth_val = pd.DataFrame(fill_NaN.transform(fifth_val))\n",
    "imputed_fifth_val.columns = fifth_val.columns\n",
    "imputed_fifth_val.index = fifth_val.index\n",
    "fifth_val = imputed_fifth_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop('OUTCOME',axis = 1)\n",
    "y_train = train.OUTCOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['MED','INITIAL_RX_AMOUNT_BILLED','INITIAL_RX_AMOUNT_PAID','INITIAL_RX_QUANTITY','INITIAL_RX_LENGTH','AGE','TOTAL_PRE_CLAIMS','PRE_NUM_POLYPHARMACY','TOTAL_RXS','PRE_RX_BILLED_SUM','PRE_RX_PAID_SUM','PRE_RX_BILLED_AVG','PRE_RX_PAID_AVG','PRE_RX_QTY_AVG','PRE_RX_DAYS_AVD','PRE_RX_QTY_SUM','PRE_RX_DAYS_SUM', \"Rural_urban_continuum_code_2013\",\"Urban_influence_code_2013\",\"Unemployment_rate_2018\",\"Median_Household_Income_2018\",\"Med_HH_Income_Percent_of_State_Total_2018\",\"PCTPOVALL_2018\",\"Percent_less_than_high_school_1418\",\"Percent_only_high_school_1418\",\"Percent_some_college_1418\",\"Percent_bachelors_or_higher_1418\",\"ELIGIBILITY_TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar.fit(x_train.loc[:,numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc[:,numeric_columns] = scalar.transform(x_train.loc[:,numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop('OUTCOME',axis = 1).astype(float)\n",
    "y_test = test.OUTCOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.loc[:,numeric_columns] = scalar.transform(x_test.loc[:,numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = fifth_val.drop('OUTCOME',axis = 1).astype(float)\n",
    "y_val = fifth_val.OUTCOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.loc[:,numeric_columns] = scalar.transform(x_val.loc[:,numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = pca.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(pca_train[y_train == 0][:,0][:1000],pca_train[y_train == 0][:,1][:1000],s = 10,marker = 'x', c = 'red')\n",
    "\n",
    "ax1.scatter(pca_train[y_train == 1][:,0][:1000],pca_train[y_train == 1][:,1][:1000],s = 10,marker = 'x', c = 'green')\n",
    "plt.ylim(-50,300)\n",
    "plt.xlim(-50,300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1.scatter(pca_train[y_train == 0][:,0][:1000],pca_train[y_train == 0][:,1][:1000],pca_train[y_train == 0][:,2][:1000],s = 1,marker = 'x', c = 'red')\n",
    "\n",
    "ax1.scatter(pca_train[y_train == 1][:,0][:1000],pca_train[y_train == 1][:,1][:1000],pca_train[y_train == 1][:,2][:1000],s = 1,marker = 'x', c = 'green')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_logreg = LogisticRegression(random_state = 42, penalty = 'l2', class_weight = 'balanced',max_iter = 1000)\n",
    "params = {'C': np.logspace(0, 4, 15),\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_clf = GridSearchCV(pca_logreg, \n",
    "                    params,\n",
    "                    n_jobs = -1,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 3,\n",
    "                    refit = False,\n",
    "                    verbose = 0,\n",
    "                    return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_clf.fit(pca_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(pca_clf.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pca_clf.cv_results_['params'][np.argmax(pca_clf.cv_results_['mean_test_score'])]\n",
    "best_params['random_state'] = 42\n",
    "print(\"Best Params\", best_params)\n",
    "\n",
    "pca_model = LogisticRegression(**best_params, penalty = 'l2',  class_weight = 'balanced',max_iter = 500)\n",
    "pca_model.fit(pca_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pca_model.predict_proba(pca_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test, y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot( recall, precision,label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot ( recall[close_default], precision[close_default],'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_hat, n_bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "\n",
    "plt.title(\"$PCA$ Calibration Curve\", fontsize=20); \n",
    "ax.set_ylabel('Fraction of Positives', fontsize = 14)\n",
    "ax.set_xlabel('Mean Predicted Value', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_file = open(STATE + \"/pca_model\", \"wb\")\n",
    "pickle.dump(pca_model, pca_file)\n",
    "pca_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state = 42, penalty = 'l2', solver = 'lbfgs', class_weight = 'balanced',max_iter = 1000)\n",
    "params = {'C': [0.001,.005,0.01,.008,.02],\n",
    "          'penalty': ['l2'],\n",
    "          'fit_intercept':[False]\n",
    "         }\n",
    "logreg_clf = GridSearchCV(logreg, \n",
    "                    params,\n",
    "                    n_jobs = -1,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 5,\n",
    "                    refit = False,\n",
    "                    verbose = 0,\n",
    "                    return_train_score = True)\n",
    "logreg_clf.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(logreg_clf.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = logreg_clf.cv_results_['params'][np.argmax(logreg_clf.cv_results_['mean_test_score'])]\n",
    "best_params['random_state'] = 42\n",
    "print(\"Best Params\", best_params)\n",
    "\n",
    "logreg = LogisticRegression(**best_params, class_weight = 'balanced',max_iter = 1000)\n",
    "logreg.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(x_test.drop(\"STATE\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = [pro[1] for pro in logreg.predict_proba(x_test.drop(\"STATE\", axis = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_true = y_test,probas_pred = y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test,y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot( recall, precision, label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot ( recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(logreg.coef_ == 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = x_train.columns[np.where(logreg.coef_ != 0)[1]]\n",
    "values = logreg.coef_[0][np.where(logreg.coef_ != 0)[1]]\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance = sorted(list(zip(labels,values)), key = lambda x: -np.abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_hat, n_bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "\n",
    "plt.title(\"$Logistic Regression$ Calibration Curve\", fontsize=20); \n",
    "ax.set_ylabel('Fraction of Positives', fontsize = 14)\n",
    "ax.set_xlabel('Mean Predicted Value', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state = 42, penalty = 'elasticnet', solver = 'saga', class_weight = 'balanced',max_iter = 1000)\n",
    "params = {'C': [.008,.02,.2],\n",
    "          'penalty': ['elasticnet'],\n",
    "          'l1_ratio': [.3,.5,.7],\n",
    "          'fit_intercept':[False]\n",
    "         }\n",
    "logreg_clf = GridSearchCV(logreg, \n",
    "                    params,\n",
    "                    n_jobs = -1,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 5,\n",
    "                    refit = False,\n",
    "                    verbose = 0,\n",
    "                    return_train_score = True)\n",
    "logreg_clf.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(logreg_clf.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = logreg_clf.cv_results_['params'][np.argmax(logreg_clf.cv_results_['mean_test_score'])]\n",
    "best_params['random_state'] = 42\n",
    "print(\"Best Params\", best_params)\n",
    "\n",
    "logreg = LogisticRegression(**best_params, solver = 'saga', class_weight = 'balanced',max_iter = 1000)\n",
    "logreg.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_file = open(STATE + \"/logreg_file\", \"wb\")\n",
    "pickle.dump(logreg, logreg_file)\n",
    "logreg_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(x_test.drop(\"STATE\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = [pro[1] for pro in logreg.predict_proba(x_test.drop(\"STATE\", axis = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_true = y_test,probas_pred = y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test,y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot( recall, precision, label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot ( recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(logreg.coef_ == 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = x_train.columns[np.where(logreg.coef_ != 0)[1]]\n",
    "values = logreg.coef_[0][np.where(logreg.coef_ != 0)[1]]\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance = sorted(list(zip(labels,values)), key = lambda x: -np.abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_hat, n_bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "\n",
    "plt.title(\"$ElasticNet$ Calibration Curve\", fontsize=20); \n",
    "ax.set_ylabel('Fraction of Positives', fontsize = 14)\n",
    "ax.set_xlabel('Mean Predicted Value', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state = 42)\n",
    "params = {'hidden_layer_sizes': [(5,2)],\n",
    "         'solver': ['sgd'],\n",
    "         'learning_rate': ['invscaling','adaptive'],\n",
    "         'activation': ['relu','tanh'],\n",
    "         'alpha': 10.0 ** -np.arange(1, 7),\n",
    "         'early_stopping': [True],\n",
    "         'learning_rate_init': [.001,.0001,.005],\n",
    "         'power_t': [.2,.5,.6],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = GridSearchCV(mlp, \n",
    "                    params,\n",
    "                    scoring = 'roc_auc',\n",
    "                    n_jobs = -1,\n",
    "                    cv = 2,\n",
    "                    refit = False,\n",
    "                    return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = mlp_clf.cv_results_['params'][np.argmax(mlp_clf.cv_results_['mean_test_score'])]\n",
    "best_params['random_state'] = 42\n",
    "print(\"Best Params\", best_params)\n",
    "\n",
    "mlp_model = MLPClassifier(**best_params)\n",
    "mlp_model.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_file = open(STATE + \"/mlp_model_file\", \"wb\")\n",
    "pickle.dump(mlp_model, mlp_model_file)\n",
    "mlp_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mlp_model.predict_proba(x_test.drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_true = y_test,probas_pred = y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test,y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot( recall, precision, label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot ( recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_hat, n_bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "\n",
    "plt.title(\"$MLP$ Calibration Curve\", fontsize=20); \n",
    "ax.set_ylabel('Fraction of Positives', fontsize = 14)\n",
    "ax.set_xlabel('Mean Predicted Value', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state = 42,subsample = .9)\n",
    "params = {'clf__gamma': [i/10.0 for i in range(4)],\n",
    "         'max_depth': range (2, 10, 1),\n",
    "         'n_estimators': range(60, 220, 80),\n",
    "         'eta': [.3, .5, .7],\n",
    "         'min_child_weight': [100],\n",
    "         'eval_metric': ['auc'],\n",
    "         'scale_pos_weight': [1,np.sum(y_train == 0)/np.sum(y_train == 1)],\n",
    "         'seed': [42]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = GridSearchCV(xgb, \n",
    "                    params,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 5,\n",
    "                    refit = False,\n",
    "                    verbose = 0,\n",
    "                    return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_clf.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(xgb_clf.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = xgb_clf.cv_results_['params'][np.argmax(xgb_clf.cv_results_['mean_test_score'])]\n",
    "best_params['random_state'] = 42\n",
    "print(\"Best Params\", best_params)\n",
    "\n",
    "xgb_model = XGBClassifier(**best_params)\n",
    "xgb_model.fit(x_train.drop(\"STATE\", axis = 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_test.drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test, y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(x_test.drop(\"STATE\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[y_test != predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_hat, n_bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.margins(x=0)\n",
    "ax.margins(y=0)\n",
    "\n",
    "plt.title(\"$XGBoost$ Calibration Curve\", fontsize=20); \n",
    "ax.set_ylabel('Fraction of Positives', fontsize = 14)\n",
    "ax.set_xlabel('Mean Predicted Value', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(xgb_model.feature_importances_)), xgb_model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = x_test.drop(\"STATE\", axis = 1).columns[np.where(xgb_model.feature_importances_)]\n",
    "values = xgb_model.feature_importances_[np.where(xgb_model.feature_importances_)]\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance = sorted(list(zip(labels,values)), key = lambda x: -np.abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [variable[0] for variable in variable_importance[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [variable[1] for variable in variable_importance[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(labels,values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_test.loc[x_test.STATE == 1].drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test.loc[x_test.STATE == 1], y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test.loc[x_test.STATE == 1], y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test.loc[x_test.STATE == 1], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.loc[x_test.STATE == 1], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test.loc[x_test.STATE == 1], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test.loc[x_test.STATE == 1], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test.loc[x_test.STATE == 1], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test.loc[x_test.STATE == 1], y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test.loc[x_test.STATE == 1], y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_test.loc[x_test.STATE == 3].drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test.loc[x_test.STATE == 3], y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test.loc[x_test.STATE == 3], y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test.loc[x_test.STATE == 3], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.loc[x_test.STATE == 3], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test.loc[x_test.STATE == 3], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test.loc[x_test.STATE == 3], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test.loc[x_test.STATE == 3], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test.loc[x_test.STATE == 3], y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test.loc[x_test.STATE == 3], y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_test.loc[x_test.STATE == 4].drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test.loc[x_test.STATE == 4], y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test.loc[x_test.STATE == 4], y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test.loc[x_test.STATE == 4], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.loc[x_test.STATE == 4], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test.loc[x_test.STATE == 4], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test.loc[x_test.STATE == 4], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test.loc[x_test.STATE == 4], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test.loc[x_test.STATE == 4], y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test.loc[x_test.STATE == 4], y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_test.loc[x_test.STATE == 5].drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_test.loc[x_test.STATE == 5], y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test.loc[x_test.STATE == 5], y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test.loc[x_test.STATE == 5], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.loc[x_test.STATE == 5], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test.loc[x_test.STATE == 5], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test.loc[x_test.STATE == 5], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test.loc[x_test.STATE == 5], y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_test.loc[x_test.STATE == 5], y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test.loc[x_test.STATE == 5], y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_model.predict_proba(x_val.drop(\"STATE\", axis = 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds_pr = sklearn.metrics.precision_recall_curve(y_val, y_hat)\n",
    "auc_prc = sklearn.metrics.auc(recall, precision)\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_val, y_hat)\n",
    "auc_roc = sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_val, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_val, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_val, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_val, y_hat > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUROC:{:.3f}\".format(auc_roc))\n",
    "print(\"AUPRC:{:.3f}\".format(auc_prc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_curve, auc\n",
    "\n",
    "avg_pre = average_precision_score(y_val, y_hat)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (12,6))\n",
    "ax1.plot(recall,precision,  label = \" average precision = {:0.2f}\".format(avg_pre), lw = 3, alpha = 0.7)\n",
    "ax1.set_ylabel('Precision', fontsize = 14)\n",
    "ax1.set_xlabel('Recall', fontsize = 14)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize = 18)\n",
    "ax1.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "ax1.plot (recall[close_default], precision[close_default], 'o', markersize = 8)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_val, y_hat)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "ax2.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc), lw = 3, alpha = 0.7)\n",
    "ax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\n",
    "ax2.set_title(\"ROC Curve\", fontsize = 18)\n",
    "\n",
    "ax2.legend(loc = 'best')\n",
    "\n",
    "#find default threshold\n",
    "close_default = np.argmin(np.abs(thresholds_roc - 0.5))\n",
    "ax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
