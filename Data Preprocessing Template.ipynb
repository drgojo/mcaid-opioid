{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import datetime\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State\n",
    "STATE = \"*****\"\n",
    "\n",
    "#Paths\n",
    "BASE_PATH = \"*****\"\n",
    "ELG_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "IND_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "PROVIDER_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "INDC_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "\n",
    "RX_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "CLAIM_COMMON_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "INSTITUTIONAL_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "DX_PATHS = [BASE_PATH + \"*****\", BASE_PATH + \"*****\"]\n",
    "\n",
    "#Times\n",
    "START_OF_RECORDS = datetime.datetime.fromisoformat(\"2015-01-01\")\n",
    "END_OF_RECORDS = datetime.datetime.fromisoformat(\"2019-09-18\")\n",
    "BACK_BUFFER = datetime.timedelta(0*30)\n",
    "FORWARD_BUFFER = datetime.timedelta(9*30)\n",
    "ACUTE_BUFFER = datetime.timedelta(3*30)\n",
    "\n",
    "#Interesting table columns\n",
    "RX_VARIABLES = ['CLAIM_ID', 'DAYS_SUPPLY_NUM', 'DISPENSE_QTY_AMT', 'FILL_NUM', 'PRODUCT_CD', 'RX_NUM', 'MA_BILLED_AMT', 'MA_PAID_AMT', 'RX_WRITTEN_DT', 'ICN_NUM', 'REFILL_AUTH_AMT']\n",
    "INSTITUTIONAL_VARIABLES = ['CLAIM_ID', 'ADMIT_DIAG_CD', 'ADMIT_DT', 'ADMIT_TYPE_CD', 'BILL_TYPE_CD','DISCHARG_DT', 'PROCEDURE_CD', 'PTNT_STATUS_CD', 'CLAIM_DOS_YR', 'ELIG_IP_DAYS_NUM', 'HOSPITAL_ADMIT_CD', 'ADMIT_HR', 'ADMIT_SOURCE_CD', 'PRIN_DIAG_CODE', 'ICN_NUM', 'SURGERY_DT', 'TRAUMA_IND']\n",
    "DX_VARIABLES = ['CLAIM_ID', 'SEQ_NUM', 'DIAG_CD', 'DIAG_QUALIFY_CD', 'CLAIM_DOS_YR', 'DIAG_TYPE_CD', 'ICN_NUM']\n",
    "IND_VARIABLES = ['INDV_ID', 'MA_NUM', 'DOB_DT', 'MA_START_DT', 'MA_STOP_DT', 'GENCD_RF', 'EGPCD_RF', 'AIDCAT_RF', 'LNGCD_RF']\n",
    "INDC_VARIABLES = ['INDV_ID','CITY_TXT','ZIP_CD']\n",
    "PROVIDER_VARIABLES = ['CLIENT_CD', 'PROVIDER_ID', 'MA_PROVIDER_ID', 'PRVTP_RF', 'SPTCD_RF', 'PROVIDER_ORG_NM', 'ORIG_PROV_SPEC_CD', 'ORIG_PROV_TYPE_CD', 'NPI_NUM', 'ORIG_PROVIDER_ID', 'PROVIDER_FILE_TYPE']\n",
    "COMMON_VARIABLES = ['SRVC_PROVIDER_NPI_ID','CLAIM_ID', 'ICN_NUM', 'ADJUST_IND', 'ADJUST_REASON_CD', 'CLAIM_FROM_DT', 'CLAIM_TYPE_CD', 'MA_BILLED_AMT', 'MA_NUM', 'MA_PAID_AMT', 'PTNT_DOB_DT', 'PTNT_GENDER_CD', 'REF_PROVIDER_ID', 'SRVC_PROVIDER_ID', 'ORIG_SRVC_PROV_TYPE_CD','SRVC_PROV_TYPE_CD', 'PTNT_PREG_IND', 'POLICY_NUM', 'PTNT_COPAY_AMT', 'CLINICAL_SIGNIF_CD', 'COUNTY_CD', 'COVERED_AMT', 'DEDUCTABLE_AMT', 'PLACE_OF_SRVC_CD', \"AID_CATEGORY_CD\", \"TRANS_STATUS_CD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NDC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13678"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numpy matrix of NDC codes and metadata for opioid drugs, as determined by the CDC\n",
    "code_reference = pd.read_csv(\"~/Resources/CDC_Opioids.csv\").values\n",
    "#Numpy array for column specific to NDC codes for opioid drugs\n",
    "numeric_opioid_ndc = code_reference[:,1]\n",
    "#Numpy matrix of NDC codes and metadata for opioid drugs with less abuse potential, as determined by the CDC\n",
    "abuse_deter_reference = pd.read_csv(\"Resources/abuse_deterent.csv\").values\n",
    "#Numpy array for column specific to NDC codes for abuse deterent opioid drugs\n",
    "numeric_deter_ndc = abuse_deter_reference[:,1]\n",
    "opioid_numeric_total = set(numeric_opioid_ndc).union(numeric_deter_ndc)\n",
    "len(opioid_numeric_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find all opioid prescriptions in the rx claims table. Ideally, we could then identify all individuals in the records with an opioid prescription Hx. However, there is no individual identifier as of now, so it's broken down into three steps: \n",
    "1. Identify all claims\n",
    "2. Map out claims to individual identifier (MA_NUM)\n",
    "3. Filter tables based on these individual identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Identify all RX claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the RX table and record all claims that have an NDC code for an opioid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed: 51200000\n",
      "Opioid Claims identified: 3783015\n"
     ]
    }
   ],
   "source": [
    "opioid_rx_claims = set()\n",
    "temp_opioid_rx_claims = []\n",
    "CHUNKSIZE = 100000\n",
    "est_rx_processed = 0\n",
    "\n",
    "for RX_PATH in RX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(RX_PATH, sep = '~', chunksize = CHUNKSIZE, usecols = ['PRODUCT_CD','CLAIM_ID'],\n",
    "                               dtype = {'PRODUCT_CD':str, 'CLAIM_ID':str}):\n",
    "        temp_opioid_rx_claims.extend(gm_chunk.loc[pd.to_numeric(gm_chunk.PRODUCT_CD.str.replace('\\D',''),errors = 'coerce').isin(opioid_numeric_total), 'CLAIM_ID'])\n",
    "        est_rx_processed += CHUNKSIZE\n",
    "\n",
    "    opioid_rx_claims = set(temp_opioid_rx_claims)\n",
    "    \n",
    "print(\"Rows processed: {}\".format(est_rx_processed))\n",
    "print(\"Opioid Claims identified: {}\".format(len(opioid_rx_claims)))\n",
    "\n",
    "del temp_opioid_rx_claims\n",
    "opioid_rx_claims_file = open(STATE + \"/opioid_rx_claims\", \"wb\")\n",
    "pickle.dump(opioid_rx_claims, opioid_rx_claims_file)\n",
    "opioid_rx_claims_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Identify all MA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 100 chunks\n",
      "Done with 200 chunks\n",
      "Rows processed: 225\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    opioid_rx_claims\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_rx_claims\",\"rb\")\n",
    "    opioid_rx_claims = pickle.load(pickle_in)\n",
    "\n",
    "    \n",
    "CHUNKSIZE = 1000000\n",
    "chunks_processed = 0\n",
    "\n",
    "pd.DataFrame(columns = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT']).to_csv(STATE + '/opioid_rx_mas.csv', index = False)\n",
    "\n",
    "for CLAIM_COMMON_PATH in CLAIM_COMMON_PATHS:\n",
    "    for gm_chunk in pd.read_csv(CLAIM_COMMON_PATH, sep = '~', chunksize = CHUNKSIZE,\n",
    "                                usecols = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT'],\n",
    "                                dtype={'MA_NUM': str, 'CLAIM_ID':str, 'CLAIM_FROM_DT':str}):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(opioid_rx_claims)].to_csv(STATE + '/opioid_rx_mas.csv', columns = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT'], mode='a',index = False, header = False)\n",
    "\n",
    "        chunks_processed += 1\n",
    "        if(chunks_processed % 100 == 0):\n",
    "            print(\"Done with {} chunks\".format(chunks_processed))\n",
    "\n",
    "print(\"Rows processed: {}\".format(chunks_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total opioid hx individuals: 3783795\n"
     ]
    }
   ],
   "source": [
    "opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "print(\"Total opioid hx individuals: {}\".format(len(opioid_rx_mas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Map MAs to list of opioid claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_rx_mas\n",
    "except NameError:\n",
    "    opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "\n",
    "mas_to_opioid_rx = {}\n",
    "for row in opioid_rx_mas.itertuples():\n",
    "    mas_to_opioid_rx.setdefault(row.MA_NUM, [])\n",
    "    mas_to_opioid_rx[row.MA_NUM].append((row.CLAIM_ID,row.CLAIM_FROM_DT))\n",
    "\n",
    "mas_to_opioid_rx = {ma: sorted(mas_to_opioid_rx[ma], key = lambda tup: tup[1]) for ma in mas_to_opioid_rx}\n",
    "\n",
    "mas_to_opioid_rx_file = open(STATE + \"/mas_to_opioid_rx\", \"wb\")\n",
    "pickle.dump(mas_to_opioid_rx, mas_to_opioid_rx_file)\n",
    "mas_to_opioid_rx_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals identified with an opioid prescription: \n",
      "484247\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of individuals identified with an opioid prescription: \\n{}\".format(opioid_rx_mas.MA_NUM.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of opioid prescriptions: 7.813754522476087\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of opioid prescriptions: {}\".format(np.mean([len(mas_to_opioid_rx[ma]) for ma in mas_to_opioid_rx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(mas_to_opioid_rx[ma]) for ma in mas_to_opioid_rx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([ma for ma in mas_to_opioid_rx if len(mas_to_opioid_rx[ma]) > 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Get Eligibility info for MAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 50 chunks\n",
      "num_elig_start_nans = 0\n",
      "num_elig_stop_nans = 306766\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mas_to_opioid_rx\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/mas_to_opioid_rx\",\"rb\")\n",
    "    mas_to_opioid_rx = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "opioid_naive_info = {}\n",
    "chunks = 0\n",
    "num_elig_start_nans = 0\n",
    "num_elig_stop_nans = 0\n",
    "\n",
    "for ELG_PATH in ELG_PATHS:\n",
    "    for gm_chunk in pd.read_csv(ELG_PATH, sep = '~', chunksize = 1000000,\n",
    "                               usecols = ['POLICY_NUM','POLICY_START_DT','POLICY_END_DT','AIDCT_RF'],\n",
    "                               dtype={'POLICY_NUM': str, 'POLICY_START_DT':str, 'POLICY_END_DT':str, 'AIDCT_RF':str}):\n",
    "        curr_filter = gm_chunk.loc[gm_chunk.POLICY_NUM.isin(mas_to_opioid_rx)]\n",
    "        for row in curr_filter.itertuples():\n",
    "            ma = row.POLICY_NUM\n",
    "            claim = mas_to_opioid_rx[row.POLICY_NUM][0][0]\n",
    "            aidcat = row.AIDCT_RF\n",
    "            initial_rx_date = datetime.datetime.fromisoformat(mas_to_opioid_rx[ma][0][1])\n",
    "\n",
    "            start_or_na = row.POLICY_START_DT\n",
    "            if (start_or_na != start_or_na):\n",
    "                elig_start = datetime.datetime(datetime.MAXYEAR,1,1)\n",
    "                num_elig_start_nans += 1\n",
    "            else:\n",
    "                elig_start = datetime.datetime.fromisoformat(start_or_na)\n",
    "\n",
    "            end_or_na = row.POLICY_END_DT\n",
    "            if (end_or_na != end_or_na):\n",
    "                elig_end = datetime.datetime(datetime.MAXYEAR,1,1)\n",
    "                num_elig_stop_nans += 1\n",
    "            else:\n",
    "                elig_end = datetime.datetime.fromisoformat(end_or_na)\n",
    "\n",
    "\n",
    "            if (elig_start <= initial_rx_date) and (elig_end >= initial_rx_date):\n",
    "                chronic_flag = -2\n",
    "                if (elig_start <= initial_rx_date - BACK_BUFFER) and (START_OF_RECORDS <= initial_rx_date - BACK_BUFFER):\n",
    "                    chronic_flag = -1\n",
    "                    if (elig_end >= initial_rx_date + FORWARD_BUFFER)  and (END_OF_RECORDS >= initial_rx_date + FORWARD_BUFFER):\n",
    "                        chronic_flag = 0\n",
    "                        for i in range(1,len(mas_to_opioid_rx[row.POLICY_NUM])):\n",
    "                            rx_next = datetime.datetime.fromisoformat(mas_to_opioid_rx[row.POLICY_NUM][i][1])\n",
    "                            if (rx_next < initial_rx_date + FORWARD_BUFFER and rx_next >= initial_rx_date + ACUTE_BUFFER):\n",
    "                                chronic_flag = 1\n",
    "                                break\n",
    "\n",
    "                opioid_naive_info.setdefault(ma, [])\n",
    "                opioid_naive_info[ma].append((elig_start, elig_end, initial_rx_date, claim, chronic_flag, aidcat))\n",
    "\n",
    "        chunks+=1\n",
    "        if(chunks%50 == 0):\n",
    "            print(\"Done with {} chunks\".format(chunks))\n",
    "        \n",
    "print('num_elig_start_nans = {}'.format(num_elig_start_nans))\n",
    "print('num_elig_stop_nans = {}'.format(num_elig_stop_nans))\n",
    "\n",
    "opioid_naive_info_file = open(STATE + \"/opioid_naive_info\", \"wb\")\n",
    "pickle.dump(opioid_naive_info, opioid_naive_info_file)\n",
    "opioid_naive_info_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_prior = 0\n",
    "missing_post = 0\n",
    "acute = 0\n",
    "chronic = 0\n",
    "for ma in opioid_naive_info:\n",
    "    flag = opioid_naive_info[ma][0][4]\n",
    "    if (flag == -2):\n",
    "        missing_prior += 1\n",
    "    elif (flag == -1):\n",
    "        missing_post += 1\n",
    "    elif (flag == 0):\n",
    "        acute += 1\n",
    "    elif (flag == 1):\n",
    "        chronic += 1\n",
    "print(\"Number of eligible opioid naive individuals = {}\".format(len(opioid_naive_info)))\n",
    "print(\"Missing Prior = {}\".format(missing_prior))\n",
    "print(\"Missing Post = {}\".format(missing_post))\n",
    "print(\"acute = {}\".format(acute))\n",
    "print(\"chronic = {}\".format(chronic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_naive_info_known = {ma: opioid_naive_info[ma] for ma in opioid_naive_info if opioid_naive_info[ma][0][4] >= 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(opioid_naive_info_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common medicaid categories before eligibility requirement:\")\n",
    "[\"{}: {:.1%}\".format(cat[0],cat[1]/len(opioid_naive_info)) for cat in Counter([opioid_naive_info[ma][0][5] for ma in opioid_naive_info]).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common medicaid categories after eligibility requirement:\")\n",
    "[\"{}: {:.1%}\".format(cat[0],cat[1]/len(opioid_naive_info_known)) for cat in Counter([opioid_naive_info_known[ma][0][5] for ma in opioid_naive_info_known]).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_naive_info_known_file = open(STATE + \"/opioid_naive_info_known\", \"wb\")\n",
    "pickle.dump(opioid_naive_info_known, opioid_naive_info_known_file)\n",
    "opioid_naive_info_known_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a filtered Claim to MA dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_rx_mas\n",
    "except NameError:\n",
    "    opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "\n",
    "try:\n",
    "    opioid_naive_info_known\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "    opioid_naive_info_known = pickle.load(pickle_in)\n",
    "    \n",
    "filter_opioid_claim_mas = opioid_rx_mas.loc[opioid_rx_mas.MA_NUM.isin(opioid_naive_info)]\n",
    "opioid_claim_ma_dict = dict(zip(filter_opioid_claim_mas.CLAIM_ID, filter_opioid_claim_mas.MA_NUM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filter tables \n",
    "using only ma's with eligibility, strip other tables to only necessary rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_naive_info_known\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "    opioid_naive_info_known = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim Common Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10 chunks\n",
      "Done with 20 chunks\n",
      "Done with 30 chunks\n",
      "Done with 40 chunks\n",
      "Done with 50 chunks\n",
      "Done with 60 chunks\n",
      "Done with 70 chunks\n",
      "Done with 80 chunks\n",
      "Done with 90 chunks\n",
      "Done with 100 chunks\n",
      "Done with 110 chunks\n",
      "Done with 120 chunks\n",
      "Done with 130 chunks\n",
      "Done with 140 chunks\n",
      "Done with 150 chunks\n",
      "Done with 160 chunks\n",
      "Done with 170 chunks\n",
      "Done with 180 chunks\n",
      "Done with 190 chunks\n",
      "Done with 200 chunks\n",
      "Done with 220 chunks\n",
      "Chunking complete: Total Records processed: 225000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (12,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#also get list of providers:\n",
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = COMMON_VARIABLES).to_csv(STATE + \"/common_filter.csv\", index = False)\n",
    "relevant_claims = set(opioid_naive_info_known.keys())\n",
    "for CLAIM_COMMON_PATH in CLAIM_COMMON_PATHS:\n",
    "    for gm_chunk in pd.read_csv(CLAIM_COMMON_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = COMMON_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.MA_NUM.isin(relevant_claims),COMMON_VARIABLES].to_csv(STATE + \"/common_filter.csv\", columns = COMMON_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "print(\"Chunking complete: Total Records processed: {}\".format(CHUNKSIZE*i))\n",
    "common_filter = pd.read_csv(STATE + \"/common_filter.csv\")\n",
    "common_filter.SRVC_PROVIDER_ID.to_csv(STATE + \"/provider_ids.csv\",index = False)\n",
    "common_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in common_filter.MA_NUM]\n",
    "common_filter.loc[:,'AIDCT_RF'] = [opioid_naive_info_known[ma][0][5] for ma in common_filter.MA_NUM]\n",
    "common_filter.to_csv(STATE + \"/common_filter.csv\",index = False)\n",
    "\n",
    "common_filter = common_filter.loc[:,['CLAIM_ID','MA_NUM']]\n",
    "\n",
    "full_claim_ma_dict = dict(zip(common_filter.CLAIM_ID, common_filter.MA_NUM))\n",
    "full_claim_ma_dict_file = open(STATE + \"/full_claim_ma_dict\", \"wb\")\n",
    "pickle.dump(full_claim_ma_dict, full_claim_ma_dict_file)\n",
    "full_claim_ma_dict_file.close()\n",
    "\n",
    "del common_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine dictionary for only claims related to eligible opioid naive ma's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RX Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10 chunks\n",
      "Done with 20 chunks\n",
      "Done with 30 chunks\n",
      "Done with 40 chunks\n",
      "Done with 50 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "rx_filter = pd.DataFrame()\n",
    "pd.DataFrame(columns = RX_VARIABLES).to_csv(STATE + \"/rx_filter.csv\", index = False)\n",
    "\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "for RX_PATH in RX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(RX_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = RX_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),RX_VARIABLES].to_csv(STATE + \"/rx_filter.csv\", columns = RX_VARIABLES, mode='a',index = False, header = False)\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "rx_filter = pd.read_csv(STATE + \"/rx_filter.csv\")\n",
    "rx_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in rx_filter.CLAIM_ID]\n",
    "rx_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in rx_filter.MA_NUM]\n",
    "rx_filter.to_csv(STATE + \"/rx_filter.csv\",index = False)\n",
    "\n",
    "del rx_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institutional Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10 chunks\n",
      "Done with 20 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (3,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = INSTITUTIONAL_VARIABLES).to_csv(STATE + \"/institutional_filter.csv\", index = False)\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "\n",
    "for INSTITUTIONAL_PATH in INSTITUTIONAL_PATHS:\n",
    "    for gm_chunk in pd.read_csv(INSTITUTIONAL_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = INSTITUTIONAL_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),INSTITUTIONAL_VARIABLES].to_csv(STATE + \"/institutional_filter.csv\", columns = INSTITUTIONAL_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "institutional_filter = pd.read_csv(STATE + \"/institutional_filter.csv\")\n",
    "institutional_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in institutional_filter.CLAIM_ID]\n",
    "institutional_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in institutional_filter.MA_NUM]\n",
    "institutional_filter.groupby('OUTCOME').mean()\n",
    "institutional_filter.to_csv(STATE + \"/institutional_filter.csv\",index = False)\n",
    "\n",
    "del institutional_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DX Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1 chunks\n",
      "Done with 2 chunks\n",
      "Done with 3 chunks\n",
      "Done with 4 chunks\n",
      "Done with 5 chunks\n",
      "Done with 6 chunks\n",
      "Done with 7 chunks\n",
      "Done with 8 chunks\n",
      "Done with 9 chunks\n",
      "Done with 10 chunks\n",
      "Done with 11 chunks\n",
      "Done with 12 chunks\n",
      "Done with 13 chunks\n",
      "Done with 14 chunks\n",
      "Done with 15 chunks\n",
      "Done with 16 chunks\n",
      "Done with 17 chunks\n",
      "Done with 18 chunks\n",
      "Done with 19 chunks\n",
      "Done with 20 chunks\n",
      "Done with 21 chunks\n",
      "Done with 22 chunks\n",
      "Done with 23 chunks\n",
      "Done with 24 chunks\n",
      "Done with 25 chunks\n",
      "Done with 26 chunks\n",
      "Done with 27 chunks\n",
      "Done with 28 chunks\n",
      "Done with 29 chunks\n",
      "Done with 30 chunks\n",
      "Done with 31 chunks\n",
      "Done with 32 chunks\n",
      "Done with 33 chunks\n",
      "Done with 34 chunks\n",
      "Done with 35 chunks\n",
      "Done with 36 chunks\n",
      "Done with 37 chunks\n",
      "Done with 38 chunks\n",
      "Done with 39 chunks\n",
      "Done with 40 chunks\n",
      "Done with 41 chunks\n",
      "Done with 42 chunks\n",
      "Done with 43 chunks\n",
      "Done with 44 chunks\n",
      "Done with 45 chunks\n",
      "Done with 46 chunks\n",
      "Done with 47 chunks\n",
      "Done with 48 chunks\n",
      "Done with 49 chunks\n",
      "Done with 50 chunks\n",
      "Done with 51 chunks\n",
      "Done with 52 chunks\n",
      "Done with 53 chunks\n",
      "Done with 54 chunks\n",
      "Done with 55 chunks\n",
      "Done with 56 chunks\n",
      "Done with 57 chunks\n",
      "Done with 58 chunks\n",
      "Done with 59 chunks\n",
      "Done with 60 chunks\n",
      "Done with 61 chunks\n",
      "Done with 62 chunks\n",
      "Done with 63 chunks\n",
      "Done with 64 chunks\n",
      "Done with 65 chunks\n",
      "Done with 66 chunks\n",
      "Done with 67 chunks\n",
      "Done with 68 chunks\n",
      "Done with 69 chunks\n",
      "Done with 70 chunks\n",
      "Done with 71 chunks\n",
      "Done with 72 chunks\n",
      "Done with 73 chunks\n",
      "Done with 74 chunks\n",
      "Done with 75 chunks\n",
      "Done with 76 chunks\n",
      "Done with 77 chunks\n",
      "Done with 78 chunks\n",
      "Done with 80 chunks\n",
      "Done with 81 chunks\n",
      "Done with 82 chunks\n",
      "Done with 83 chunks\n",
      "Done with 84 chunks\n",
      "Done with 85 chunks\n",
      "Done with 86 chunks\n",
      "Done with 87 chunks\n",
      "Done with 88 chunks\n",
      "Done with 89 chunks\n",
      "Done with 90 chunks\n",
      "Done with 91 chunks\n",
      "Done with 92 chunks\n",
      "Done with 93 chunks\n",
      "Done with 94 chunks\n",
      "Done with 95 chunks\n",
      "Done with 96 chunks\n",
      "Done with 97 chunks\n",
      "Done with 98 chunks\n",
      "Done with 99 chunks\n",
      "Done with 100 chunks\n",
      "Done with 101 chunks\n",
      "Done with 102 chunks\n",
      "Done with 103 chunks\n",
      "Done with 104 chunks\n",
      "Done with 105 chunks\n",
      "Done with 106 chunks\n",
      "Done with 107 chunks\n",
      "Done with 108 chunks\n",
      "Done with 109 chunks\n",
      "Done with 110 chunks\n",
      "Done with 111 chunks\n",
      "Done with 112 chunks\n",
      "Done with 113 chunks\n",
      "Done with 114 chunks\n",
      "Done with 115 chunks\n",
      "Done with 116 chunks\n",
      "Done with 117 chunks\n",
      "Done with 118 chunks\n",
      "Done with 119 chunks\n",
      "Done with 120 chunks\n",
      "Done with 121 chunks\n",
      "Done with 122 chunks\n",
      "Done with 123 chunks\n",
      "Done with 124 chunks\n",
      "Done with 125 chunks\n",
      "Done with 126 chunks\n",
      "Done with 127 chunks\n",
      "Done with 128 chunks\n",
      "Done with 129 chunks\n",
      "Done with 130 chunks\n",
      "Done with 131 chunks\n",
      "Done with 132 chunks\n",
      "Done with 133 chunks\n",
      "Done with 134 chunks\n",
      "Done with 135 chunks\n",
      "Done with 136 chunks\n",
      "Done with 137 chunks\n",
      "Done with 138 chunks\n",
      "Done with 139 chunks\n",
      "Done with 140 chunks\n",
      "Done with 141 chunks\n",
      "Done with 142 chunks\n",
      "Done with 143 chunks\n",
      "Done with 144 chunks\n",
      "Done with 145 chunks\n",
      "Done with 146 chunks\n",
      "Done with 147 chunks\n",
      "Done with 148 chunks\n",
      "Done with 149 chunks\n",
      "Done with 150 chunks\n",
      "Done with 151 chunks\n",
      "Done with 152 chunks\n",
      "Done with 153 chunks\n",
      "Done with 154 chunks\n",
      "Done with 155 chunks\n",
      "Done with 156 chunks\n",
      "Done with 157 chunks\n",
      "Done with 158 chunks\n",
      "Done with 159 chunks\n",
      "Done with 160 chunks\n",
      "Done with 161 chunks\n",
      "Done with 162 chunks\n",
      "Done with 163 chunks\n",
      "Done with 164 chunks\n",
      "Done with 165 chunks\n",
      "Done with 166 chunks\n",
      "Done with 167 chunks\n",
      "Done with 168 chunks\n",
      "Done with 169 chunks\n",
      "Done with 170 chunks\n",
      "Done with 171 chunks\n",
      "Done with 172 chunks\n",
      "Done with 173 chunks\n",
      "Done with 174 chunks\n",
      "Done with 175 chunks\n",
      "Done with 176 chunks\n",
      "Done with 177 chunks\n",
      "Done with 178 chunks\n",
      "Done with 179 chunks\n",
      "Done with 180 chunks\n",
      "Done with 181 chunks\n",
      "Done with 182 chunks\n",
      "Done with 183 chunks\n",
      "Done with 184 chunks\n",
      "Done with 185 chunks\n",
      "Done with 186 chunks\n",
      "Done with 187 chunks\n",
      "Done with 188 chunks\n",
      "Done with 189 chunks\n",
      "Done with 190 chunks\n",
      "Done with 191 chunks\n",
      "Done with 192 chunks\n",
      "Done with 193 chunks\n",
      "Done with 194 chunks\n",
      "Done with 195 chunks\n",
      "Done with 196 chunks\n",
      "Done with 197 chunks\n",
      "Done with 198 chunks\n",
      "Done with 199 chunks\n",
      "Done with 200 chunks\n",
      "Done with 201 chunks\n",
      "Done with 202 chunks\n",
      "Done with 203 chunks\n",
      "Done with 204 chunks\n",
      "Done with 205 chunks\n",
      "Done with 206 chunks\n",
      "Done with 207 chunks\n",
      "Done with 208 chunks\n",
      "Done with 209 chunks\n",
      "Done with 210 chunks\n",
      "Done with 211 chunks\n",
      "Done with 212 chunks\n",
      "Done with 213 chunks\n",
      "Done with 214 chunks\n",
      "Done with 215 chunks\n",
      "Done with 216 chunks\n",
      "Done with 217 chunks\n",
      "Done with 218 chunks\n",
      "Done with 219 chunks\n",
      "Done with 220 chunks\n",
      "Done with 221 chunks\n",
      "Done with 222 chunks\n",
      "Done with 223 chunks\n",
      "Done with 224 chunks\n",
      "Done with 225 chunks\n",
      "Done with 226 chunks\n",
      "Done with 227 chunks\n",
      "Done with 228 chunks\n",
      "Done with 229 chunks\n",
      "Done with 230 chunks\n",
      "Done with 231 chunks\n",
      "Done with 232 chunks\n",
      "Done with 233 chunks\n",
      "Done with 234 chunks\n",
      "Done with 235 chunks\n",
      "Done with 236 chunks\n",
      "Done with 237 chunks\n",
      "Done with 238 chunks\n",
      "Done with 239 chunks\n",
      "Done with 240 chunks\n",
      "Done with 241 chunks\n",
      "Done with 242 chunks\n",
      "Done with 243 chunks\n",
      "Done with 244 chunks\n",
      "Done with 245 chunks\n",
      "Done with 246 chunks\n",
      "Done with 247 chunks\n",
      "Done with 248 chunks\n",
      "Done with 249 chunks\n",
      "Done with 250 chunks\n",
      "Done with 251 chunks\n",
      "Done with 252 chunks\n",
      "Done with 253 chunks\n",
      "Done with 254 chunks\n",
      "Done with 255 chunks\n",
      "Done with 256 chunks\n",
      "Done with 257 chunks\n",
      "Done with 258 chunks\n",
      "Done with 259 chunks\n",
      "Done with 260 chunks\n",
      "Done with 261 chunks\n",
      "Done with 262 chunks\n",
      "Done with 263 chunks\n",
      "Done with 264 chunks\n",
      "Done with 265 chunks\n",
      "Done with 266 chunks\n",
      "Done with 267 chunks\n",
      "Done with 268 chunks\n",
      "Done with 269 chunks\n",
      "Done with 270 chunks\n",
      "Done with 271 chunks\n",
      "Done with 272 chunks\n",
      "Done with 273 chunks\n",
      "Done with 274 chunks\n",
      "Done with 275 chunks\n",
      "Done with 276 chunks\n",
      "Done with 277 chunks\n",
      "Done with 278 chunks\n",
      "Done with 279 chunks\n",
      "Done with 280 chunks\n",
      "Done with 281 chunks\n",
      "Done with 282 chunks\n",
      "Done with 283 chunks\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = DX_VARIABLES).to_csv(STATE + \"/dx_filter.csv\", index = False)\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "\n",
    "for DX_PATH in DX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(DX_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = DX_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),DX_VARIABLES].to_csv(STATE + \"/dx_filter.csv\", columns = DX_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 1 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "dx_filter = pd.read_csv(STATE + \"/dx_filter.csv\")\n",
    "dx_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in dx_filter.CLAIM_ID]\n",
    "dx_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in dx_filter.MA_NUM]\n",
    "\n",
    "dx_filter.to_csv(STATE + \"/dx_filter.csv\",index = False)\n",
    "\n",
    "del dx_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IND Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1 chunks\n",
      "Done with 2 chunks\n",
      "Done with 3 chunks\n",
      "Done with 4 chunks\n",
      "Done with 5 chunks\n",
      "Done with 6 chunks\n",
      "Done with 7 chunks\n",
      "Done with 8 chunks\n",
      "Done with 9 chunks\n",
      "Done with 10 chunks\n",
      "Done with 11 chunks\n",
      "Done with 12 chunks\n",
      "Done with 13 chunks\n",
      "Done with 14 chunks\n",
      "Done with 15 chunks\n",
      "Done with 16 chunks\n",
      "Done with 17 chunks\n",
      "Done with 18 chunks\n",
      "Done with 19 chunks\n",
      "Done with 20 chunks\n",
      "Done with 21 chunks\n",
      "Done with 22 chunks\n",
      "Done with 23 chunks\n",
      "Done with 24 chunks\n",
      "Done with 25 chunks\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = IND_VARIABLES).to_csv(STATE + \"/ind_filter.csv\", index = False)\n",
    "relevant_claims = set(opioid_naive_info_known.keys())\n",
    "for IND_PATH in IND_PATHS:\n",
    "    for gm_chunk in pd.read_csv(IND_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = IND_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.MA_NUM.isin(relevant_claims),IND_VARIABLES].to_csv(STATE + \"/ind_filter.csv\", columns = IND_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 1 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "ind_filter = pd.read_csv(STATE + \"/ind_filter.csv\")\n",
    "ind_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in ind_filter.MA_NUM]\n",
    "ind_filter.groupby('OUTCOME').mean()\n",
    "ind_filter.to_csv(STATE + \"/ind_filter.csv\",index = False)\n",
    "\n",
    "del ind_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDC Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10 chunks\n",
      "Done with 20 chunks\n",
      "Done with 30 chunks\n",
      "Done with 40 chunks\n",
      "Done with 50 chunks\n",
      "Done with 60 chunks\n",
      "Done with 70 chunks\n",
      "Done with 80 chunks\n",
      "Done with 90 chunks\n",
      "Done with 100 chunks\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = INDC_VARIABLES).to_csv(STATE + \"/indc_filter.csv\", index = False)\n",
    "indv_ids = set(pd.read_csv(STATE + \"/ind_filter.csv\").INDV_ID)\n",
    "for INDC_PATH in INDC_PATHS:\n",
    "    for gm_chunk in pd.read_csv(INDC_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = INDC_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.INDV_ID.isin(indv_ids),INDC_VARIABLES].to_csv(STATE + \"/indc_filter.csv\", columns = INDC_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "provider_ids = pd.read_csv(STATE + \"/provider_ids.csv\").SRVC_PROVIDER_ID\n",
    "\n",
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "provider_filter = pd.DataFrame()\n",
    "for PROVIDER_PATH in PROVIDER_PATHS:\n",
    "    for gm_chunk in pd.read_csv(PROVIDER_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = PROVIDER_VARIABLES):\n",
    "        provider_filter = provider_filter.append(gm_chunk.loc[gm_chunk.PROVIDER_ID.isin(provider_ids),PROVIDER_VARIABLES], ignore_index=True)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "provider_filter.to_csv(STATE + \"/provider_filter.csv\",index = False)\n",
    "\n",
    "del provider_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files you should end up with at the end of this step:\n",
    "provider_filter.csv <br />\n",
    "provider_ids.csv <br />\n",
    "common_filter.csv <br />\n",
    "ind_filter.csv <br />\n",
    "dx_filter.csv <br />\n",
    "institutional_filter.csv <br />\n",
    "rx_filter.csv <br />\n",
    "opioid_claim_ma_dict (pickled dictionary of opioid claims to their ma's) <br />\n",
    "opioid_naive_info (pickled dictionary of opioid naive ma's to a list of their opioid rx claims and the corresponding dates, and eligibility periods of initial rx <br />\n",
    "mas_to_opioid_rx (pickled dict of mas to their opioid claim numbers) <br />\n",
    "opioid_rx_mas.csv (opioid claim to ma csv file) <br />\n",
    "opioid_rx_claims (pickled set of opioid claims) <br />\n",
    "full_claim_ma_dict (all claims for all ma's with opioid rx hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_naive_info\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info\",\"rb\")\n",
    "    opioid_naive_info = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number in each class:\n",
    "Counter([opioid_naive_info[ma][0][4] for ma in opioid_naive_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_time = [x/30 for x in [min((opioid_naive_info[ma][0][2] - opioid_naive_info[ma][0][0]).days, (opioid_naive_info[ma][0][2] - START_OF_RECORDS).days) for ma in opioid_naive_info] ]\n",
    "post_time = [x/30 for x in [min((opioid_naive_info[ma][0][1] - opioid_naive_info[ma][0][2]).days, (END_OF_RECORDS - opioid_naive_info[ma][0][2]).days) for ma in opioid_naive_info] ]\n",
    "outcomes = [opioid_naive_info[ma][0][4] for ma in opioid_naive_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pre_time, bins = 100)\n",
    "plt.axvline(x=3, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(post_time, bins = 100)\n",
    "plt.axvline(x=9, color = 'red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
