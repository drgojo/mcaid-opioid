{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import datetime\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State\n",
    "STATE = \"***\"\n",
    "\n",
    "#Paths\n",
    "BASE_PATH = \"***\"\n",
    "ELG_PATHS = [BASE_PATH + \"***\"]\n",
    "IND_PATHS = [BASE_PATH + \"***\"]\n",
    "PROVIDER_PATHS = [BASE_PATH + \"***\"]\n",
    "INDC_PATHS = [BASE_PATH + \"***\"]\n",
    "\n",
    "RX_PATHS = [BASE_PATH + \"***\"]\n",
    "CLAIM_COMMON_PATHS = [BASE_PATH + \"***\"]\n",
    "INSTITUTIONAL_PATHS = [BASE_PATH + \"***\"]\n",
    "DX_PATHS = [BASE_PATH + \"***.txt\"]\n",
    "\n",
    "#Times\n",
    "START_OF_RECORDS = datetime.datetime.fromisoformat(\"2015-01-01\")\n",
    "END_OF_RECORDS = datetime.datetime.fromisoformat(\"2019-09-18\")\n",
    "BACK_BUFFER = datetime.timedelta(0*30)\n",
    "FORWARD_BUFFER = datetime.timedelta(9*30)\n",
    "ACUTE_BUFFER = datetime.timedelta(3*30)\n",
    "\n",
    "#Interesting table columns\n",
    "RX_VARIABLES = ['CLAIM_ID', 'DAYS_SUPPLY_NUM', 'DISPENSE_QTY_AMT', 'FILL_NUM', 'PRODUCT_CD', 'RX_NUM', 'MA_BILLED_AMT', 'MA_PAID_AMT', 'RX_WRITTEN_DT', 'ICN_NUM', 'REFILL_AUTH_AMT']\n",
    "INSTITUTIONAL_VARIABLES = ['CLAIM_ID', 'ADMIT_DIAG_CD', 'ADMIT_DT', 'ADMIT_TYPE_CD', 'BILL_TYPE_CD','DISCHARG_DT', 'PROCEDURE_CD', 'PTNT_STATUS_CD', 'CLAIM_DOS_YR', 'ELIG_IP_DAYS_NUM', 'HOSPITAL_ADMIT_CD', 'ADMIT_HR', 'ADMIT_SOURCE_CD', 'PRIN_DIAG_CODE', 'ICN_NUM', 'SURGERY_DT', 'TRAUMA_IND']\n",
    "DX_VARIABLES = ['CLAIM_ID', 'SEQ_NUM', 'DIAG_CD', 'DIAG_QUALIFY_CD', 'CLAIM_DOS_YR', 'DIAG_TYPE_CD', 'ICN_NUM']\n",
    "IND_VARIABLES = ['INDV_ID', 'MA_NUM', 'DOB_DT', 'MA_START_DT', 'MA_STOP_DT', 'GENCD_RF', 'EGPCD_RF', 'AIDCAT_RF', 'LNGCD_RF']\n",
    "INDC_VARIABLES = ['INDV_ID','CITY_TXT','ZIP_CD']\n",
    "PROVIDER_VARIABLES = ['CLIENT_CD', 'PROVIDER_ID', 'MA_PROVIDER_ID', 'PRVTP_RF', 'SPTCD_RF', 'PROVIDER_ORG_NM', 'ORIG_PROV_SPEC_CD', 'ORIG_PROV_TYPE_CD', 'NPI_NUM', 'ORIG_PROVIDER_ID', 'PROVIDER_FILE_TYPE']\n",
    "COMMON_VARIABLES = ['SRVC_PROVIDER_NPI_ID','CLAIM_ID', 'ICN_NUM', 'ADJUST_IND', 'ADJUST_REASON_CD', 'CLAIM_FROM_DT', 'CLAIM_TYPE_CD', 'MA_BILLED_AMT', 'MA_NUM', 'MA_PAID_AMT', 'PTNT_DOB_DT', 'PTNT_GENDER_CD', 'REF_PROVIDER_ID', 'SRVC_PROVIDER_ID', 'ORIG_SRVC_PROV_TYPE_CD','SRVC_PROV_TYPE_CD', 'PTNT_PREG_IND', 'POLICY_NUM', 'PTNT_COPAY_AMT', 'CLINICAL_SIGNIF_CD', 'COUNTY_CD', 'COVERED_AMT', 'DEDUCTABLE_AMT', 'PLACE_OF_SRVC_CD', \"AID_CATEGORY_CD\", \"TRANS_STATUS_CD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " try:\n",
    "    opioid_naive_info_known\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "    opioid_naive_info_known = pickle.load(pickle_in)\n",
    "    \n",
    "\n",
    "common_filter = pd.read_csv(STATE + \"/common_filter.csv\", usecols = ['CLAIM_ID','CLAIM_TYPE_CD','SRVC_PROVIDER_ID','ORIG_SRVC_PROV_TYPE_CD','PTNT_PREG_IND','COUNTY_CD','PLACE_OF_SRVC_CD','OUTCOME','MA_NUM','CLAIM_FROM_DT'])\n",
    "common_filter.CLAIM_FROM_DT = common_filter.CLAIM_FROM_DT.map(lambda x: datetime.datetime.fromisoformat(x))\n",
    "\n",
    "\n",
    "#Filter for prescriptions that are before the rx date and after the 6 months before rx date\n",
    "common_filter_pre_period = common_filter[([x[0][2] for x in common_filter.MA_NUM.map(opioid_naive_info_known)] > common_filter.CLAIM_FROM_DT)\n",
    "                                & ([x[0][2]  - BACK_BUFFER for x in common_filter.MA_NUM.map(opioid_naive_info_known)] < common_filter.CLAIM_FROM_DT)]\n",
    "common_filter_pre_period = common_filter_pre_period.loc[common_filter_pre_period.MA_NUM.isin(opioid_naive_info_known.keys())]\n",
    "\n",
    "del common_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NDC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Numpy matrix of NDC codes and metadata for opioid drugs, as determined by the CDC\n",
    "code_reference = pd.read_csv(\"~/Resources/CDC_Opioids.csv\").values\n",
    "#Numpy array for column specific to NDC codes for opioid drugs\n",
    "numeric_opioid_ndc = code_reference[:,1]\n",
    "#Numpy matrix of NDC codes and metadata for opioid drugs with less abuse potential, as determined by the CDC\n",
    "abuse_deter_reference = pd.read_csv(\"Resources/abuse_deterent.csv\").values\n",
    "#Numpy array for column specific to NDC codes for abuse deterent opioid drugs\n",
    "numeric_deter_ndc = abuse_deter_reference[:,1]\n",
    "opioid_numeric_total = set(numeric_opioid_ndc).union(numeric_deter_ndc)\n",
    "len(opioid_numeric_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find all opioid prescriptions in the rx claims table. Ideally, we could then identify all individuals in the records with an opioid prescription Hx. However, there is no individual identifier as of now, so it's broken down into three steps: \n",
    "1. Identify all claims\n",
    "2. Map out claims to individual identifier (MA_NUM as of now)\n",
    "3. Filter tables based on these individual identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Identify all RX claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the RX table and record all claims that have an NDC code for an opioid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_rx_claims = set()\n",
    "temp_opioid_rx_claims = []\n",
    "CHUNKSIZE = 100000\n",
    "est_rx_processed = 0\n",
    "\n",
    "for RX_PATH in RX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(RX_PATH, sep = '~', chunksize = CHUNKSIZE, usecols = ['PRODUCT_CD','CLAIM_ID'],\n",
    "                               dtype = {'PRODUCT_CD':str, 'CLAIM_ID':str}):\n",
    "        temp_opioid_rx_claims.extend(gm_chunk.loc[pd.to_numeric(gm_chunk.PRODUCT_CD.str.replace('\\D',''),errors = 'coerce').isin(opioid_numeric_total), 'CLAIM_ID'])\n",
    "        est_rx_processed += CHUNKSIZE\n",
    "\n",
    "    opioid_rx_claims = set(temp_opioid_rx_claims)\n",
    "    \n",
    "print(\"Rows processed: {}\".format(est_rx_processed))\n",
    "print(\"Opioid Claims identified: {}\".format(len(opioid_rx_claims)))\n",
    "\n",
    "del temp_opioid_rx_claims\n",
    "opioid_rx_claims_file = open(STATE + \"/opioid_rx_claims\", \"wb\")\n",
    "pickle.dump(opioid_rx_claims, opioid_rx_claims_file)\n",
    "opioid_rx_claims_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Identify all MA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_rx_claims\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_rx_claims\",\"rb\")\n",
    "    opioid_rx_claims = pickle.load(pickle_in)\n",
    "\n",
    "    \n",
    "CHUNKSIZE = 1000000\n",
    "chunks_processed = 0\n",
    "\n",
    "pd.DataFrame(columns = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT']).to_csv(STATE + '/opioid_rx_mas.csv', index = False)\n",
    "\n",
    "for CLAIM_COMMON_PATH in CLAIM_COMMON_PATHS:\n",
    "    for gm_chunk in pd.read_csv(CLAIM_COMMON_PATH, sep = '~', chunksize = CHUNKSIZE,\n",
    "                                usecols = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT'],\n",
    "                                dtype={'MA_NUM': str, 'CLAIM_ID':str, 'CLAIM_FROM_DT':str}):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(opioid_rx_claims)].to_csv(STATE + '/opioid_rx_mas.csv', columns = ['MA_NUM','CLAIM_ID','CLAIM_FROM_DT'], mode='a',index = False, header = False)\n",
    "\n",
    "        chunks_processed += 1\n",
    "        if(chunks_processed % 100 == 0):\n",
    "            print(\"Done with {} chunks\".format(chunks_processed))\n",
    "\n",
    "print(\"Rows processed: {}\".format(chunks_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "print(\"Total opioid hx individuals: {}\".format(len(opioid_rx_mas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Map MAs to list of opioid claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_rx_mas\n",
    "except NameError:\n",
    "    opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "\n",
    "mas_to_opioid_rx = {}\n",
    "for row in opioid_rx_mas.itertuples():\n",
    "    mas_to_opioid_rx.setdefault(row.MA_NUM, [])\n",
    "    mas_to_opioid_rx[row.MA_NUM].append((row.CLAIM_ID,row.CLAIM_FROM_DT))\n",
    "\n",
    "mas_to_opioid_rx = {ma: sorted(mas_to_opioid_rx[ma], key = lambda tup: tup[1]) for ma in mas_to_opioid_rx}\n",
    "\n",
    "mas_to_opioid_rx_file = open(STATE + \"/mas_to_opioid_rx\", \"wb\")\n",
    "pickle.dump(mas_to_opioid_rx, mas_to_opioid_rx_file)\n",
    "mas_to_opioid_rx_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of individuals identified with an opioid prescription: \\n{}\".format(opioid_rx_mas.MA_NUM.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of opioid prescriptions: {}\".format(np.mean([len(mas_to_opioid_rx[ma]) for ma in mas_to_opioid_rx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(mas_to_opioid_rx[ma]) for ma in mas_to_opioid_rx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Get Eligibility info for MAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mas_to_opioid_rx\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/mas_to_opioid_rx\",\"rb\")\n",
    "    mas_to_opioid_rx = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "opioid_naive_info = {}\n",
    "chunks = 0\n",
    "num_elig_start_nans = 0\n",
    "num_elig_stop_nans = 0\n",
    "\n",
    "for ELG_PATH in ELG_PATHS:\n",
    "    for gm_chunk in pd.read_csv(ELG_PATH, sep = '~', chunksize = 1000000,\n",
    "                               usecols = ['POLICY_NUM','POLICY_START_DT','POLICY_END_DT','AIDCT_RF'],\n",
    "                               dtype={'POLICY_NUM': str, 'POLICY_START_DT':str, 'POLICY_END_DT':str, 'AIDCT_RF':str}):\n",
    "        curr_filter = gm_chunk.loc[gm_chunk.POLICY_NUM.isin(mas_to_opioid_rx)]\n",
    "        for row in curr_filter.itertuples():\n",
    "            ma = row.POLICY_NUM\n",
    "            claim = mas_to_opioid_rx[row.POLICY_NUM][0][0]\n",
    "            aidcat = row.AIDCT_RF\n",
    "            initial_rx_date = datetime.datetime.fromisoformat(mas_to_opioid_rx[ma][0][1])\n",
    "\n",
    "            start_or_na = row.POLICY_START_DT\n",
    "            if (start_or_na != start_or_na):\n",
    "                elig_start = datetime.datetime(datetime.MAXYEAR,1,1)\n",
    "                num_elig_start_nans += 1\n",
    "            else:\n",
    "                elig_start = datetime.datetime.fromisoformat(start_or_na)\n",
    "\n",
    "            end_or_na = row.POLICY_END_DT\n",
    "            if (end_or_na != end_or_na):\n",
    "                elig_end = datetime.datetime(datetime.MAXYEAR,1,1)\n",
    "                num_elig_stop_nans += 1\n",
    "            else:\n",
    "                elig_end = datetime.datetime.fromisoformat(end_or_na)\n",
    "\n",
    "\n",
    "            if (elig_start <= initial_rx_date) and (elig_end >= initial_rx_date):\n",
    "                chronic_flag = -2\n",
    "                if (elig_start <= initial_rx_date - BACK_BUFFER) and (START_OF_RECORDS <= initial_rx_date - BACK_BUFFER):\n",
    "                    chronic_flag = -1\n",
    "                    if (elig_end >= initial_rx_date + FORWARD_BUFFER)  and (END_OF_RECORDS >= initial_rx_date + FORWARD_BUFFER):\n",
    "                        chronic_flag = 0\n",
    "                        for i in range(1,len(mas_to_opioid_rx[row.POLICY_NUM])):\n",
    "                            rx_next = datetime.datetime.fromisoformat(mas_to_opioid_rx[row.POLICY_NUM][i][1])\n",
    "                            if (rx_next < initial_rx_date + FORWARD_BUFFER and rx_next >= initial_rx_date + ACUTE_BUFFER):\n",
    "                                chronic_flag = 1\n",
    "                                break\n",
    "\n",
    "                opioid_naive_info.setdefault(ma, [])\n",
    "                opioid_naive_info[ma].append((elig_start, elig_end, initial_rx_date, claim, chronic_flag, aidcat))\n",
    "\n",
    "        chunks+=1\n",
    "        if(chunks%50 == 0):\n",
    "            print(\"Done with {} chunks\".format(chunks))\n",
    "        \n",
    "print('num_elig_start_nans = {}'.format(num_elig_start_nans))\n",
    "print('num_elig_stop_nans = {}'.format(num_elig_stop_nans))\n",
    "\n",
    "opioid_naive_info_file = open(STATE + \"/opioid_naive_info\", \"wb\")\n",
    "pickle.dump(opioid_naive_info, opioid_naive_info_file)\n",
    "opioid_naive_info_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_prior = 0\n",
    "missing_post = 0\n",
    "acute = 0\n",
    "chronic = 0\n",
    "for ma in opioid_naive_info:\n",
    "    flag = opioid_naive_info[ma][0][4]\n",
    "    if (flag == -2):\n",
    "        missing_prior += 1\n",
    "    elif (flag == -1):\n",
    "        missing_post += 1\n",
    "    elif (flag == 0):\n",
    "        acute += 1\n",
    "    elif (flag == 1):\n",
    "        chronic += 1\n",
    "print(\"Number of eligible opioid naive individuals = {}\".format(len(opioid_naive_info)))\n",
    "print(\"Missing Prior = {}\".format(missing_prior))\n",
    "print(\"Missing Post = {}\".format(missing_post))\n",
    "print(\"acute = {}\".format(acute))\n",
    "print(\"chronic = {}\".format(chronic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_naive_info_known = {ma: opioid_naive_info[ma] for ma in opioid_naive_info if opioid_naive_info[ma][0][4] >= 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(opioid_naive_info_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common medicaid categories before eligibility requirement:\")\n",
    "[\"{}: {:.1%}\".format(cat[0],cat[1]/len(opioid_naive_info)) for cat in Counter([opioid_naive_info[ma][0][5] for ma in opioid_naive_info]).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common medicaid categories after eligibility requirement:\")\n",
    "[\"{}: {:.1%}\".format(cat[0],cat[1]/len(opioid_naive_info_known)) for cat in Counter([opioid_naive_info_known[ma][0][5] for ma in opioid_naive_info_known]).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioid_naive_info_known_file = open(STATE + \"/opioid_naive_info_known\", \"wb\")\n",
    "pickle.dump(opioid_naive_info_known, opioid_naive_info_known_file)\n",
    "opioid_naive_info_known_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a filtered Claim to MA dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_rx_mas\n",
    "except NameError:\n",
    "    opioid_rx_mas = pd.read_csv(STATE + \"/opioid_rx_mas.csv\")\n",
    "\n",
    "try:\n",
    "    opioid_naive_info_known\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "    opioid_naive_info_known = pickle.load(pickle_in)\n",
    "    \n",
    "filter_opioid_claim_mas = opioid_rx_mas.loc[opioid_rx_mas.MA_NUM.isin(opioid_naive_info)]\n",
    "opioid_claim_ma_dict = dict(zip(filter_opioid_claim_mas.CLAIM_ID, filter_opioid_claim_mas.MA_NUM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filter tables \n",
    "using only ma's with eligibility, strip other tables to only necessary rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opioid_naive_info_known\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/opioid_naive_info_known\",\"rb\")\n",
    "    opioid_naive_info_known = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim Common Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#also get list of providers:\n",
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = COMMON_VARIABLES).to_csv(STATE + \"/common_filter.csv\", index = False)\n",
    "relevant_claims = set(opioid_naive_info_known.keys())\n",
    "for CLAIM_COMMON_PATH in CLAIM_COMMON_PATHS:\n",
    "    for gm_chunk in pd.read_csv(CLAIM_COMMON_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = COMMON_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.MA_NUM.isin(relevant_claims),COMMON_VARIABLES].to_csv(STATE + \"/common_filter.csv\", columns = COMMON_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "print(\"Chunking complete: Total Records processed: {}\".format(CHUNKSIZE*i))\n",
    "common_filter = pd.read_csv(STATE + \"/common_filter.csv\")\n",
    "common_filter.SRVC_PROVIDER_ID.to_csv(STATE + \"/provider_ids.csv\",index = False)\n",
    "common_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in common_filter.MA_NUM]\n",
    "common_filter.loc[:,'AIDCT_RF'] = [opioid_naive_info_known[ma][0][5] for ma in common_filter.MA_NUM]\n",
    "common_filter.to_csv(STATE + \"/common_filter.csv\",index = False)\n",
    "\n",
    "common_filter = common_filter.loc[:,['CLAIM_ID','MA_NUM']]\n",
    "\n",
    "full_claim_ma_dict = dict(zip(common_filter.CLAIM_ID, common_filter.MA_NUM))\n",
    "full_claim_ma_dict_file = open(STATE + \"/full_claim_ma_dict\", \"wb\")\n",
    "pickle.dump(full_claim_ma_dict, full_claim_ma_dict_file)\n",
    "full_claim_ma_dict_file.close()\n",
    "\n",
    "del common_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine dictionary for only claims related to eligible opioid naive ma's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RX Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "rx_filter = pd.DataFrame()\n",
    "pd.DataFrame(columns = RX_VARIABLES).to_csv(STATE + \"/rx_filter.csv\", index = False)\n",
    "\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "for RX_PATH in RX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(RX_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = RX_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),RX_VARIABLES].to_csv(STATE + \"/rx_filter.csv\", columns = RX_VARIABLES, mode='a',index = False, header = False)\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "rx_filter = pd.read_csv(STATE + \"/rx_filter.csv\")\n",
    "rx_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in rx_filter.CLAIM_ID]\n",
    "rx_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in rx_filter.MA_NUM]\n",
    "rx_filter.to_csv(STATE + \"/rx_filter.csv\",index = False)\n",
    "\n",
    "del rx_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institutional Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = INSTITUTIONAL_VARIABLES).to_csv(STATE + \"/institutional_filter.csv\", index = False)\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "\n",
    "for INSTITUTIONAL_PATH in INSTITUTIONAL_PATHS:\n",
    "    for gm_chunk in pd.read_csv(INSTITUTIONAL_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = INSTITUTIONAL_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),INSTITUTIONAL_VARIABLES].to_csv(STATE + \"/institutional_filter.csv\", columns = INSTITUTIONAL_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "institutional_filter = pd.read_csv(STATE + \"/institutional_filter.csv\")\n",
    "institutional_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in institutional_filter.CLAIM_ID]\n",
    "institutional_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in institutional_filter.MA_NUM]\n",
    "institutional_filter.groupby('OUTCOME').mean()\n",
    "institutional_filter.to_csv(STATE + \"/institutional_filter.csv\",index = False)\n",
    "\n",
    "del institutional_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DX Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_claim_ma_dict\n",
    "except NameError:\n",
    "    pickle_in = open(STATE + \"/full_claim_ma_dict\",\"rb\")\n",
    "    full_claim_ma_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = DX_VARIABLES).to_csv(STATE + \"/dx_filter.csv\", index = False)\n",
    "relevant_claims = set(full_claim_ma_dict.keys())\n",
    "\n",
    "for DX_PATH in DX_PATHS:\n",
    "    for gm_chunk in pd.read_csv(DX_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = DX_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.CLAIM_ID.isin(relevant_claims),DX_VARIABLES].to_csv(STATE + \"/dx_filter.csv\", columns = DX_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 1 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "\n",
    "dx_filter = pd.read_csv(STATE + \"/dx_filter.csv\")\n",
    "dx_filter.loc[:,'MA_NUM'] = [full_claim_ma_dict[claim] for claim in dx_filter.CLAIM_ID]\n",
    "dx_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in dx_filter.MA_NUM]\n",
    "\n",
    "dx_filter.to_csv(STATE + \"/dx_filter.csv\",index = False)\n",
    "\n",
    "del dx_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IND Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = IND_VARIABLES).to_csv(STATE + \"/ind_filter.csv\", index = False)\n",
    "relevant_claims = set(opioid_naive_info_known.keys())\n",
    "for IND_PATH in IND_PATHS:\n",
    "    for gm_chunk in pd.read_csv(IND_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = IND_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.MA_NUM.isin(relevant_claims),IND_VARIABLES].to_csv(STATE + \"/ind_filter.csv\", columns = IND_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 1 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "ind_filter = pd.read_csv(STATE + \"/ind_filter.csv\")\n",
    "ind_filter.loc[:,'OUTCOME'] = [opioid_naive_info_known[ma][0][4] for ma in ind_filter.MA_NUM]\n",
    "ind_filter.groupby('OUTCOME').mean()\n",
    "ind_filter.to_csv(STATE + \"/ind_filter.csv\",index = False)\n",
    "\n",
    "del ind_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDC Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "pd.DataFrame(columns = INDC_VARIABLES).to_csv(STATE + \"/indc_filter.csv\", index = False)\n",
    "indv_ids = set(pd.read_csv(STATE + \"/ind_filter.csv\").INDV_ID)\n",
    "for INDC_PATH in INDC_PATHS:\n",
    "    for gm_chunk in pd.read_csv(INDC_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = INDC_VARIABLES):\n",
    "        gm_chunk.loc[gm_chunk.INDV_ID.isin(indv_ids),INDC_VARIABLES].to_csv(STATE + \"/indc_filter.csv\", columns = INDC_VARIABLES, mode='a',index = False, header = False)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "provider_ids = pd.read_csv(STATE + \"/provider_ids.csv\").SRVC_PROVIDER_ID\n",
    "\n",
    "CHUNKSIZE = 1000000\n",
    "i = 0\n",
    "provider_filter = pd.DataFrame()\n",
    "for PROVIDER_PATH in PROVIDER_PATHS:\n",
    "    for gm_chunk in pd.read_csv(PROVIDER_PATH, sep = '~', chunksize = CHUNKSIZE, low_memory=False, usecols = PROVIDER_VARIABLES):\n",
    "        provider_filter = provider_filter.append(gm_chunk.loc[gm_chunk.PROVIDER_ID.isin(provider_ids),PROVIDER_VARIABLES], ignore_index=True)\n",
    "\n",
    "        i += 1\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Done with {} chunks\".format(i))\n",
    "        \n",
    "provider_filter.to_csv(STATE + \"/provider_filter.csv\",index = False)\n",
    "\n",
    "del provider_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files you should end up with at the end of this step:\n",
    "provider_filter.csv <br />\n",
    "provider_ids.csv <br />\n",
    "common_filter.csv <br />\n",
    "ind_filter.csv <br />\n",
    "dx_filter.csv <br />\n",
    "institutional_filter.csv <br />\n",
    "rx_filter.csv <br />\n",
    "opioid_claim_ma_dict (pickled dictionary of opioid claims to their ma's) <br />\n",
    "opioid_naive_info (pickled dictionary of opioid naive ma's to a list of their opioid rx claims and the corresponding dates, and eligibility periods of initial rx <br />\n",
    "mas_to_opioid_rx (pickled dict of mas to their opioid claim numbers) <br />\n",
    "opioid_rx_mas.csv (opioid claim to ma csv file) <br />\n",
    "opioid_rx_claims (pickled set of opioid claims) <br />\n",
    "full_claim_ma_dict (all claims for all ma's with opioid rx hx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
